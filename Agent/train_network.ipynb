{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from trade_sim import Market\n",
    "import tensorflow.contrib as contrib\n",
    "import os\n",
    "from multiprocessing import Process, Queue, Manager\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, numberOfCurrencies, sess, timeFrame=50,  initialPortfolio=10000.0):\n",
    "        self._s = sess\n",
    "        self._isTraining = tf.placeholder(tf.bool)\n",
    "        self.inputT = tf.placeholder(shape=[None, numberOfCurrencies, timeFrame, 3], dtype=tf.float32)\n",
    "        self.model = tf.layers.conv2d(inputs=self.inputT, filters=10, kernel_size=[1,3], activation=tf.nn.leaky_relu)\n",
    "        self.model = tf.layers.conv2d(inputs=self.model, filters=10, kernel_size=[1,48], activation=tf.nn.leaky_relu)\n",
    "        #self.model = tf.layers.conv2d(inputs=self.model, filters=100, kernel_size=[1,22], activation=tf.nn.leaky_relu)\n",
    "\n",
    "      #  self.conv1 = tf.nn.depthwise_conv2d(self.inputT, [1,3,3,4], [1,1,1,1], 'SAME')\n",
    "        #self.conv2 = tf.layers.conv2d(inputs=self.conv1, filters=150, kernel_size=[1,8], activation=tf.nn.relu)\n",
    "        #self.conv3 = tf.layers.conv2d(inputs = self.conv2, filters=200, kernel_size=[1,41], activation=tf.nn.relu)\n",
    "        #self.conv4 = tf.layers.conv2d(inputs=self.conv3, filters=30, kernel_size=[1,1] , activation=tf.nn.relu) \n",
    "#        self.model = tf.layers.dense(self.model, 2000, activation=tf.nn.leaky_relu)\n",
    "        self.model = tf.layers.conv2d(inputs=self.model, filters=100, kernel_size=[1,1], activation=tf.nn.leaky_relu)\n",
    "        self.model = tf.layers.dense(self.model, 1000)\n",
    "        #self.model = tf.layers.dropout(self.model, training = self._isTraining)\n",
    "        print(self.model.shape)\n",
    "        self.model = tf.layers.dense(self.model, 1)\n",
    "        print(self.model.shape)\n",
    "        self._allocate = tf.nn.softmax(self.model, axis=1)\n",
    "        \n",
    "        self.priceChanges = tf.placeholder(shape=[None, numberOfCurrencies, 1], dtype=tf.float32)\n",
    "        \n",
    "        #self.loss = -tf.matmul(tf.matrix_transpose(tf.nn.leaky_relu(tf.log(self.priceChanges), alpha=10)),tf.reshape(self._allocate, [-1, numberOfCurrencies, 1]))\n",
    "        self.averageLoss = tf.reduce_mean(tf.matmul(tf.matrix_transpose(self.priceChanges), \n",
    "                                             tf.scalar_mul(tf.constant(initialPortfolio), \n",
    "                                               tf.reshape(self._allocate, [-1, numberOfCurrencies, 1]))))\n",
    "\n",
    "        self.loss = tf.exp(tf.reduce_sum(tf.multiply(-tf.log(self.priceChanges), tf.reshape(self._allocate, [tf.shape(self._allocate)[0], numberOfCurrencies, 1]))))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        self._train = self.optimizer.minimize(self.loss)\n",
    "        \n",
    "    def act(self, observation):\n",
    "        return self._s.run(self._allocate, feed_dict={self.inputT: observation})\n",
    "    \n",
    "    def train_step(self, obs, prices):\n",
    "        batch_feed = {self.inputT : obs,\n",
    "                     self.priceChanges: prices,\n",
    "                      self._isTraining: True\n",
    "                     }\n",
    "        _, lossValue = self._s.run([self._train, self.averageLoss], feed_dict=batch_feed)\n",
    "        return lossValue\n",
    "    def test_model(self, obs, prices):\n",
    "        print(self._s.run([self.averageLoss, self._allocate], feed_dict={self.inputT : obs, self.priceChanges: prices, self._isTraining: False}))\n",
    "        print(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    def __init__(self, batch_size=1, batch_count=0, period_size=50, batch_offset=0):\n",
    "        self.batchPath = os.path.abspath(\"./Batches\")\n",
    "        self.labelPath = os.path.abspath(\"./Labels\")\n",
    "        self.threads = 8\n",
    "        self.batch = batch_count\n",
    "        self.batch_size = batch_size\n",
    "        self.period_size = period_size\n",
    "        self.offset = batch_offset\n",
    "        \n",
    "            \n",
    "    def importData(self, simulator):\n",
    "        testSim = simulator\n",
    "        manager = Manager()\n",
    "        d = manager.list()\n",
    "        q = Queue()\n",
    "        jobs = []\n",
    "        PERIOD_SIZE = self.period_size\n",
    "        BATCH_SIZE = self.batch_size\n",
    "        #BATCH_COUNT = int(math.floor(minSize / BATCH_SIZE))\n",
    "        if self.batch == 0:\n",
    "            minSize = math.inf\n",
    "            for pair in simulator.df.values():\n",
    "                if len(pair.index) < minSize:\n",
    "                    minSize = len(pair.index)\n",
    "            print(minSize)\n",
    "            self.batch = int(math.floor(minSize / self.batch_size))\n",
    "        BATCH_COUNT = self.batch\n",
    "        BATCH_OFFSET = self.offset\n",
    "        dates = testSim.getAllDates()\n",
    "        index = list(range(BATCH_COUNT))\n",
    "        feed = []\n",
    "        \n",
    "\n",
    "        running = False\n",
    "        count = 0\n",
    "        countFile = 0\n",
    "#         while 1:\n",
    "#             if count < self.threads and len(index) != 0:\n",
    "#                 for i in random.sample(index, self.threads-count if len(index) >= self.threads-count else len(index)):\n",
    "#                     p = Process(target=testSim.processTimePeriod, args=(d, PERIOD_SIZE, dates, BATCH_SIZE * (i + BATCH_OFFSET) + PERIOD_SIZE, BATCH_SIZE))\n",
    "#                     jobs.append(p)\n",
    "#                     p.start()\n",
    "#                     index.remove(i)\n",
    "#                 print(\"Total unstarted:{}\".format(len(index)))\n",
    "#             count = 0\n",
    "#             for p in jobs:\n",
    "#                 if not p.is_alive():\n",
    "#                     p.terminate()\n",
    "#                     jobs.remove(p)\n",
    "#                 else:\n",
    "#                     count += 1\n",
    "#             while len(d) != 0:\n",
    "# #                feed.append(q.get())\n",
    "#                 pair = d.pop(0)\n",
    "#                 np.save((os.path.join(self.batchPath, \"Batch_\" + str(countFile))), pair[0])\n",
    "#                 np.save((os.path.join(self.labelPath, \"Label_\" + str(countFile))), pair[1])\n",
    "#                 countFile += 1\n",
    "#             if count == 0 and len(index) == 0 and countFile == BATCH_COUNT:\n",
    "#                 break\n",
    "        prevIndex = [-1 for i in range(len(simulator.currencies))]\n",
    "        for i in index:\n",
    "            prevIndex = testSim.processTimePeriod(d, PERIOD_SIZE, dates, BATCH_SIZE * (i+BATCH_OFFSET) + PERIOD_SIZE, BATCH_SIZE, prevIndex)\n",
    "            pair = d.pop(0)\n",
    "           # print('END OF CALL ---------------------')\n",
    "            np.save((os.path.join(self.batchPath, \"Batch_\" + str(countFile))), pair[0])\n",
    "            np.save((os.path.join(self.labelPath, \"Label_\" + str(countFile))), pair[1])\n",
    "            countFile += 1\n",
    "        \n",
    "#         print(len(feed))\n",
    "#         return feed\n",
    "    \n",
    "    def prepareData(self, simulator, reset=False):\n",
    "        if not os.path.exists(self.batchPath):\n",
    "            os.makedirs(self.batchPath)\n",
    "        if not os.path.exists(self.labelPath):\n",
    "            os.makedirs(self.labelPath)\n",
    "        if reset:\n",
    "            for folder in (self.batchPath, self.labelPath):\n",
    "                for the_file in os.listdir(folder):\n",
    "                    file_path = os.path.join(folder, the_file)\n",
    "                    try:\n",
    "                        if os.path.isfile(file_path):\n",
    "                            os.unlink(file_path)\n",
    "                    except:\n",
    "                        pass\n",
    "        if \"Done\" in os.listdir(self.batchPath):\n",
    "            return\n",
    "        now = time.time()\n",
    "        fullData = self.importData(simulator)\n",
    "        later = time.time()\n",
    "        print('Total time for all data:{} seconds'.format(int(later-now)))\n",
    "        count = 0\n",
    "#         for pair in fullData:\n",
    "#             np.save((os.path.join(self.batchPath, \"Batch_\" + str(count))), pair[0])\n",
    "#             np.save((os.path.join(self.labelPath, \"Label_\" + str(count))), pair[1])\n",
    "#             count += 1\n",
    "        f = open(os.path.join(self.batchPath, \"Done\"), 'w')\n",
    "        f.close()\n",
    "        \n",
    "    def readData(self):\n",
    "        availableData = os.listdir(self.batchPath)\n",
    "        if \"Done\" in availableData:\n",
    "            availableData.remove(\"Done\")\n",
    "        indices = range(len(availableData))\n",
    "        n_tasks = len(availableData)/self.threads\n",
    "        print(n_tasks)\n",
    "        q = Queue()\n",
    "        if n_tasks == 0:\n",
    "            return\n",
    "        offSet = 0\n",
    "        jobs = []\n",
    "        feed = []\n",
    "        for i in range(self.threads):\n",
    "            p = Process(target=self.readFiles, args=(int(offSet), int(offSet+n_tasks), q))\n",
    "            jobs.append(p)\n",
    "            print(\"Starting\")\n",
    "            p.start() \n",
    "            offSet += n_tasks\n",
    "        while not q.empty() or len(feed) != self.batch:\n",
    "            feed.append(q.get())\n",
    "        return feed\n",
    "            \n",
    "        \n",
    "    def readFiles(self, fileRangeStart, fileRangeEnd):\n",
    "        listL = []\n",
    "        for i in range(fileRangeStart, fileRangeEnd):\n",
    "            listL.append((str(i), np.load(os.path.join(self.batchPath, \"Batch_\" + str(i) + \".npy\")), np.load(os.path.join(self.labelPath, \"Label_\"+ str(i) + \".npy\"))))\n",
    "        return listL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSim = Market(['EUR','USD', 'TRY', 'JPY'], os.path.abspath('../Data_Processing/ProcessedData'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for all data:848 seconds\n"
     ]
    }
   ],
   "source": [
    "data = DataManager(batch_size = 50, batch_count = 10000, batch_offset=0)\n",
    "data.prepareData(testSim, reset=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedBackup = data.readFiles(0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array_equal(test2[0][2], test1[0][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4, 1, 1000)\n",
      "(?, 4, 1, 1)\n",
      "Episode: 0\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    feed = feedBackup.copy()\n",
    "    seeds = [3, 5, 7]\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "    tf.set_random_seed(seeds[0])\n",
    "    test1 = Agent(len(testSim.currencies), sess)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    testData = feed[5]\n",
    "    feed.pop(5)\n",
    "    #print(testData)\n",
    "    prices = []\n",
    "    changes = []\n",
    "    for data in feed:\n",
    "        prices.append(data[1])\n",
    "        changes.append(data[2])\n",
    "        if np.isinf(data[1]).any() == True:\n",
    "            print(\"FOUND in price\")\n",
    "        if np.isinf(data[2]).any() == True:\n",
    "            print('FOUND in change')\n",
    "    batch_size = 1000\n",
    "    \n",
    "#     testArr = np.zeros((4,1))\n",
    "#     testArr[0] = -1\n",
    "#     print(np.any(testArr < 0))\n",
    "#     print(testSim.df['USDTRY'].iloc[323850:323949].values)\n",
    "#     return\n",
    "    \n",
    "    priceMatrix = np.concatenate(prices)\n",
    "    changeMatrix = np.concatenate(changes)\n",
    "#     indexCheck = 3238\n",
    "    #print(priceMatrix[indexCheck*batch_size:indexCheck*batch_size+batch_size][21])\n",
    "    #print(changeMatrix[indexCheck*batch_size:indexCheck*batch_size+batch_size][21])\n",
    "#     return\n",
    "    indexSize = priceMatrix.shape[0]/batch_size\n",
    "    try:\n",
    "        for episode in range(50):\n",
    "            print(\"Episode: {}\".format(episode))\n",
    "            index = list(range(int(indexSize)))\n",
    "            batches = len(index)\n",
    "            loss = 0\n",
    "            count = len(feed)\n",
    "            while len(index) != 0:\n",
    "                for i in random.sample(index, 1):\n",
    "                    with tf.device('/gpu:0'):\n",
    "                        addToLoss = test1.train_step(priceMatrix[i*batch_size:i*batch_size + batch_size], changeMatrix[i*batch_size:i*batch_size + batch_size])\n",
    "                        loss += addToLoss\n",
    "                        if addToLoss < 0 or addToLoss == np.nan:\n",
    "                            print('Loss at index {}: {}'.format(i,addToLoss))\n",
    "                    index.remove(i)\n",
    "            print(loss/batches)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Ending\")\n",
    "        test1.test_model(testData[1], testData[2])\n",
    "    finally:\n",
    "        print(\"Ending\")\n",
    "        test1.test_model(testData[1], testData[2])\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
